temperature: 1.0
max_tokens: 128
top_p: 1.0
model_name: "llama"
api_provider: "localhost"